{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277,
     "referenced_widgets": [
      "f6605510567842a492c401714ce42394",
      "d0628d1eabde42a4a5fe253a144b407f",
      "eca41f4d142b47d49949f5dd7e6cbd7e",
      "b7ca89f2db6f4ac59279d372b21d577c",
      "13bbaec00a1b461eadc3d76332a9a28c",
      "2954f5babcbf428f98c2c5bbe8131f14",
      "4695b53553da42f4b7879b75a3447ea4",
      "a7d48fdb6f7f41f688067adacecf148e",
      "995c00f0fb5041b583369fcb91064182",
      "74cefdce9f004af2b224be374a81e82a",
      "51cc9fdf79234b38b5244d86eed49fc4",
      "d9f8fb5c66634b7dbd992c479537cc23",
      "01d3e7103f4b4af7bfd7c9e918dc2052",
      "1420adc3e2774c2eae232b96c464cfa8",
      "1e6413c9e6064e3aa1212b091ac2fe34",
      "147e688e48a2491299824e8607004ecb",
      "c9346dba73764ba685773cd97b6df191",
      "37fad0649a6f4de987764bd2d1b94bc2",
      "99bc44ede7d04ca89ec0dfc7f3290fa9",
      "8d85210daf8e4e8ab4f5525dc2d9a852",
      "17a8a9c979e74761a09936f26bbc5f03",
      "5d4819da817b4fbcb58ac77c540022d1",
      "b76dde963d214d42877bdc03c7d4bfc2",
      "30dba4dafbef442ca70affaf63a50fc8",
      "f43527d9e6ba457cbc3eee528a5eb12c",
      "043efb72c6fd480aa87f2419b519f5b3",
      "adc683d643cb485b8aa40b64bab49ba3",
      "ad5fd0d8952845bf922b89b9f3672c04",
      "7106908618994d19a6a218e7bc2eb113",
      "20dfb7ddf3c44845bdab00eea6b32781",
      "73a775ecd84244b5b4d70f5cb33ca7b2",
      "663f3cc2d4524643ac41cb0c0e1cfa49",
      "fdd4c58ea4074d619e85e5e801c6926d",
      "ef7231fc7747475191e074913df7770b",
      "29d0e3ca4cd24b80b526cbfcb10c92fe",
      "b8acecbab4d146b0b1a9dab75fca7a68",
      "ea6d4b393066463b9bd260baeb1bf4bb",
      "bbd8594426c84a0aa52089e7826a555b",
      "0130e81b6e294563b1b26f4f4e9aae61",
      "febf804c96f24d9390c3aed6e7be19fe",
      "926c297536384dfab4540d2e4edac333",
      "f4503c90ac194459ac8a3ce96d3799d1",
      "1a20bbc34d9f40fb8b204b3f342f8c2a",
      "dbc12228ac92426bb12c2b150e03ee29"
     ]
    },
    "executionInfo": {
     "elapsed": 6951,
     "status": "ok",
     "timestamp": 1747928802245,
     "user": {
      "displayName": "Saadhika Kodati Y",
      "userId": "15997902867013533304"
     },
     "user_tz": -330
    },
    "id": "VaqMZSwAcvWD",
    "outputId": "b553abfb-85f2-4ab0-c7fe-7a2a5e480cde"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6605510567842a492c401714ce42394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h1>Dataset Fairness Analysis Tool</h1>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ca89f2db6f4ac59279d372b21d577c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(HTML(value='<b>Step 1:</b> Upload your dataset (CSV or Excel file)'), FileUpload(â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import io\n",
    "from google.colab import files\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FairnessAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.sensitive_attr = None\n",
    "        self.target = None\n",
    "        self.model = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.encoders = {}\n",
    "        self.categorical_columns = []\n",
    "        self.numerical_columns = []\n",
    "        self.predictions = None\n",
    "        self.fairness_metrics = {}\n",
    "\n",
    "    def load_data(self, file):\n",
    "        \"\"\"Load the dataset from uploaded file\"\"\"\n",
    "        try:\n",
    "            # Determine file type by extension\n",
    "            if file.name.endswith('.csv'):\n",
    "                self.data = pd.read_csv(io.BytesIO(file.content))\n",
    "            elif file.name.endswith(('.xls', '.xlsx')):\n",
    "                self.data = pd.read_excel(io.BytesIO(file.content))\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file format. Please upload a CSV or Excel file.\")\n",
    "\n",
    "            # Identify categorical and numerical columns\n",
    "            self.categorical_columns = self.data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "            self.numerical_columns = self.data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "            print(f\"Dataset loaded successfully with {self.data.shape[0]} rows and {self.data.shape[1]} columns.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return False\n",
    "\n",
    "    def preprocess_data(self, sensitive_attr, target, test_size=0.3):\n",
    "        \"\"\"Preprocess the dataset for fairness analysis\"\"\"\n",
    "        self.sensitive_attr = sensitive_attr\n",
    "        self.target = target\n",
    "\n",
    "        # Handle missing values\n",
    "        for column in self.data.columns:\n",
    "            if self.data[column].dtype in ['int64', 'float64']:\n",
    "                self.data[column].fillna(self.data[column].median(), inplace=True)\n",
    "            else:\n",
    "                self.data[column].fillna(self.data[column].mode()[0], inplace=True)\n",
    "\n",
    "        # Encode categorical variables\n",
    "        for column in self.categorical_columns:\n",
    "            if column != target:  # Don't encode the target yet\n",
    "                le = LabelEncoder()\n",
    "                self.data[column] = le.fit_transform(self.data[column])\n",
    "                self.encoders[column] = le\n",
    "\n",
    "        # Encode target if it's categorical\n",
    "        if target in self.categorical_columns:\n",
    "            le = LabelEncoder()\n",
    "            self.data[target] = le.fit_transform(self.data[target])\n",
    "            self.encoders[target] = le\n",
    "\n",
    "        # Create features and target\n",
    "        X = self.data.drop(columns=[target])\n",
    "        y = self.data[target]\n",
    "\n",
    "        # Split data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        print(f\"Data preprocessed successfully. Training set: {self.X_train.shape[0]} samples, Test set: {self.X_test.shape[0]} samples.\")\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"Train a RandomForest model\"\"\"\n",
    "        self.model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        self.predictions = self.model.predict(self.X_test)\n",
    "\n",
    "        # Calculate overall accuracy\n",
    "        accuracy = accuracy_score(self.y_test, self.predictions)\n",
    "        print(f\"Model trained successfully with accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    def calculate_fairness_metrics(self):\n",
    "        \"\"\"Calculate fairness metrics including demographic parity\"\"\"\n",
    "        sensitive_values = self.X_test[self.sensitive_attr].unique()\n",
    "        metrics = {}\n",
    "\n",
    "        # Overall accuracy\n",
    "        overall_accuracy = accuracy_score(self.y_test, self.predictions)\n",
    "        metrics[\"overall_accuracy\"] = overall_accuracy\n",
    "\n",
    "        # Group-specific metrics\n",
    "        group_metrics = {}\n",
    "        for value in sensitive_values:\n",
    "            mask = self.X_test[self.sensitive_attr] == value\n",
    "\n",
    "            # Group accuracy\n",
    "            group_accuracy = accuracy_score(self.y_test[mask], self.predictions[mask])\n",
    "\n",
    "            # Positive prediction rate (demographic parity metric)\n",
    "            positive_rate = np.mean(self.predictions[mask] == 1)\n",
    "\n",
    "            # True positive rate (equal opportunity metric)\n",
    "            if sum(self.y_test[mask] == 1) > 0:\n",
    "                tpr = sum((self.predictions[mask] == 1) & (self.y_test[mask] == 1)) / sum(self.y_test[mask] == 1)\n",
    "            else:\n",
    "                tpr = 0\n",
    "\n",
    "            # False positive rate (predictive equality metric)\n",
    "            if sum(self.y_test[mask] == 0) > 0:\n",
    "                fpr = sum((self.predictions[mask] == 1) & (self.y_test[mask] == 0)) / sum(self.y_test[mask] == 0)\n",
    "            else:\n",
    "                fpr = 0\n",
    "\n",
    "            group_metrics[value] = {\n",
    "                \"accuracy\": group_accuracy,\n",
    "                \"positive_rate\": positive_rate,\n",
    "                \"true_positive_rate\": tpr,\n",
    "                \"false_positive_rate\": fpr\n",
    "            }\n",
    "\n",
    "        metrics[\"group_metrics\"] = group_metrics\n",
    "\n",
    "        # Calculate demographic parity difference (absolute difference in positive prediction rates)\n",
    "        pos_rates = [metrics[\"group_metrics\"][v][\"positive_rate\"] for v in sensitive_values]\n",
    "        dp_diff = max(pos_rates) - min(pos_rates)\n",
    "        metrics[\"demographic_parity_difference\"] = dp_diff\n",
    "\n",
    "        # Equal opportunity difference (absolute difference in true positive rates)\n",
    "        tpr_values = [metrics[\"group_metrics\"][v][\"true_positive_rate\"] for v in sensitive_values]\n",
    "        eop_diff = max(tpr_values) - min(tpr_values)\n",
    "        metrics[\"equal_opportunity_difference\"] = eop_diff\n",
    "\n",
    "        # Equalized odds difference (maximum difference across TPR and FPR)\n",
    "        fpr_values = [metrics[\"group_metrics\"][v][\"false_positive_rate\"] for v in sensitive_values]\n",
    "        eo_diff = max(max(tpr_values) - min(tpr_values), max(fpr_values) - min(fpr_values))\n",
    "        metrics[\"equalized_odds_difference\"] = eo_diff\n",
    "\n",
    "        self.fairness_metrics = metrics\n",
    "\n",
    "        # Interpret demographic parity\n",
    "        if dp_diff < 0.05:\n",
    "            dp_interpretation = \"Excellent fairness (nearly equal outcomes across groups)\"\n",
    "        elif dp_diff < 0.1:\n",
    "            dp_interpretation = \"Good fairness (small difference in outcomes across groups)\"\n",
    "        elif dp_diff < 0.2:\n",
    "            dp_interpretation = \"Moderate bias detected (noticeable difference in outcomes across groups)\"\n",
    "        else:\n",
    "            dp_interpretation = \"Significant bias detected (large difference in outcomes across groups)\"\n",
    "\n",
    "        self.fairness_metrics[\"dp_interpretation\"] = dp_interpretation\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def visualize_results(self):\n",
    "        \"\"\"Create visualizations for the fairness analysis\"\"\"\n",
    "        if not self.fairness_metrics:\n",
    "            print(\"No fairness metrics calculated yet.\")\n",
    "            return\n",
    "\n",
    "        # Set up the figure\n",
    "        plt.figure(figsize=(20, 16))\n",
    "\n",
    "        # 1. Accuracy comparison across groups\n",
    "        plt.subplot(2, 2, 1)\n",
    "        group_values = list(self.fairness_metrics[\"group_metrics\"].keys())\n",
    "        group_names = []\n",
    "\n",
    "        # Try to decode group names if they were encoded\n",
    "        if self.sensitive_attr in self.encoders:\n",
    "            try:\n",
    "                group_names = [self.encoders[self.sensitive_attr].inverse_transform([val])[0] for val in group_values]\n",
    "            except:\n",
    "                group_names = [f\"{self.sensitive_attr}_{val}\" for val in group_values]\n",
    "        else:\n",
    "            group_names = [f\"{self.sensitive_attr}_{val}\" for val in group_values]\n",
    "\n",
    "        accuracies = [self.fairness_metrics[\"group_metrics\"][val][\"accuracy\"] for val in group_values]\n",
    "\n",
    "        sns.barplot(x=group_names, y=accuracies)\n",
    "        plt.axhline(y=self.fairness_metrics[\"overall_accuracy\"], color='r', linestyle='--',\n",
    "                  label=f'Overall Accuracy: {self.fairness_metrics[\"overall_accuracy\"]:.4f}')\n",
    "        plt.title('Accuracy by Group')\n",
    "        plt.xlabel(self.sensitive_attr)\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend()\n",
    "\n",
    "        # 2. Positive prediction rates (Demographic Parity)\n",
    "        plt.subplot(2, 2, 2)\n",
    "        pos_rates = [self.fairness_metrics[\"group_metrics\"][val][\"positive_rate\"] for val in group_values]\n",
    "        sns.barplot(x=group_names, y=pos_rates)\n",
    "        plt.title(f'Positive Prediction Rate by Group\\nDP Difference: {self.fairness_metrics[\"demographic_parity_difference\"]:.4f}')\n",
    "        plt.xlabel(self.sensitive_attr)\n",
    "        plt.ylabel('Positive Prediction Rate')\n",
    "        plt.ylim(0, 1)\n",
    "\n",
    "        # 3. True Positive Rates (Equal Opportunity)\n",
    "        plt.subplot(2, 2, 3)\n",
    "        tpr_values = [self.fairness_metrics[\"group_metrics\"][val][\"true_positive_rate\"] for val in group_values]\n",
    "        sns.barplot(x=group_names, y=tpr_values)\n",
    "        plt.title(f'True Positive Rate by Group\\nEO Difference: {self.fairness_metrics[\"equal_opportunity_difference\"]:.4f}')\n",
    "        plt.xlabel(self.sensitive_attr)\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.ylim(0, 1)\n",
    "\n",
    "        # 4. False Positive Rates (Predictive Equality)\n",
    "        plt.subplot(2, 2, 4)\n",
    "        fpr_values = [self.fairness_metrics[\"group_metrics\"][val][\"false_positive_rate\"] for val in group_values]\n",
    "        sns.barplot(x=group_names, y=fpr_values)\n",
    "        plt.title('False Positive Rate by Group')\n",
    "        plt.xlabel(self.sensitive_attr)\n",
    "        plt.ylabel('False Positive Rate')\n",
    "        plt.ylim(0, 1)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Display feature importance\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        features = self.X_train.columns\n",
    "        importances = self.model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "\n",
    "        plt.title('Feature Importances')\n",
    "        plt.bar(range(len(indices)), importances[indices], align='center')\n",
    "        plt.xticks(range(len(indices)), [features[i] for i in indices], rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"Generate a text summary of the fairness analysis\"\"\"\n",
    "        if not self.fairness_metrics:\n",
    "            return \"No fairness metrics calculated yet.\"\n",
    "\n",
    "        report = \"\"\"\n",
    "        # Fairness Analysis Summary Report\n",
    "\n",
    "        ## Overall Model Performance\n",
    "        - Accuracy: {:.4f}\n",
    "\n",
    "        ## Fairness Metrics\n",
    "        - Demographic Parity Difference: {:.4f} ({})\n",
    "        - Equal Opportunity Difference: {:.4f}\n",
    "        - Equalized Odds Difference: {:.4f}\n",
    "\n",
    "        ## Group-Specific Metrics\n",
    "        \"\"\".format(\n",
    "            self.fairness_metrics[\"overall_accuracy\"],\n",
    "            self.fairness_metrics[\"demographic_parity_difference\"],\n",
    "            self.fairness_metrics[\"dp_interpretation\"],\n",
    "            self.fairness_metrics[\"equal_opportunity_difference\"],\n",
    "            self.fairness_metrics[\"equalized_odds_difference\"]\n",
    "        )\n",
    "\n",
    "        # Add group-specific metrics\n",
    "        for group, metrics in self.fairness_metrics[\"group_metrics\"].items():\n",
    "            if self.sensitive_attr in self.encoders:\n",
    "                try:\n",
    "                    group_name = self.encoders[self.sensitive_attr].inverse_transform([[group]])[0]\n",
    "                except:\n",
    "                    group_name = f\"{self.sensitive_attr}_{group}\"\n",
    "            else:\n",
    "                group_name = f\"{self.sensitive_attr}_{group}\"\n",
    "\n",
    "            report += \"\"\"\n",
    "        ### Group: {}\n",
    "        - Accuracy: {:.4f}\n",
    "        - Positive Prediction Rate: {:.4f}\n",
    "        - True Positive Rate: {:.4f}\n",
    "        - False Positive Rate: {:.4f}\n",
    "            \"\"\".format(\n",
    "                group_name,\n",
    "                metrics[\"accuracy\"],\n",
    "                metrics[\"positive_rate\"],\n",
    "                metrics[\"true_positive_rate\"],\n",
    "                metrics[\"false_positive_rate\"]\n",
    "            )\n",
    "\n",
    "        # Add interpretation\n",
    "        report += \"\"\"\n",
    "        ## Interpretation\n",
    "\n",
    "        {}\n",
    "\n",
    "        ### Recommendations:\n",
    "        \"\"\".format(self.fairness_metrics[\"dp_interpretation\"])\n",
    "\n",
    "        if self.fairness_metrics[\"demographic_parity_difference\"] > 0.1:\n",
    "            report += \"\"\"\n",
    "        - Consider applying fairness constraints during model training\n",
    "        - Examine potential sources of bias in the dataset\n",
    "        - Collect more representative data for underrepresented groups\n",
    "        - Consider feature engineering to reduce reliance on biased features\n",
    "            \"\"\"\n",
    "        else:\n",
    "            report += \"\"\"\n",
    "        - Continue monitoring fairness metrics as the model is updated\n",
    "        - Consider performing additional fairness analyses on other sensitive attributes\n",
    "            \"\"\"\n",
    "\n",
    "        return report\n",
    "\n",
    "# Create the interactive app\n",
    "def create_fairness_app():\n",
    "    analyzer = FairnessAnalyzer()\n",
    "\n",
    "    # Step 1: Upload dataset\n",
    "    step1_output = widgets.Output()\n",
    "    upload_instructions = widgets.HTML(\"<b>Step 1:</b> Upload your dataset (CSV or Excel file)\")\n",
    "    file_upload = widgets.FileUpload(accept='.csv, .xlsx, .xls', multiple=False, description='Upload File')\n",
    "\n",
    "    # Step 2: Select attributes\n",
    "    step2_output = widgets.Output()\n",
    "    sensitive_attr_dropdown = widgets.Dropdown(description='Sensitive Attribute:')\n",
    "    target_dropdown = widgets.Dropdown(description='Target Variable:')\n",
    "    preprocess_button = widgets.Button(description=\"Preprocess Data\", disabled=True)\n",
    "\n",
    "    # Step 3: Analyze fairness\n",
    "    step3_output = widgets.Output()\n",
    "    analyze_button = widgets.Button(description=\"Analyze Fairness\", disabled=True)\n",
    "    save_model_button = widgets.Button(description=\"Save Model\", disabled=True)\n",
    "\n",
    "    # File upload handler\n",
    "    def on_upload_change(change):\n",
    "        with step1_output:\n",
    "            clear_output()\n",
    "            if file_upload.value:\n",
    "                try:\n",
    "                    # Get the uploaded file\n",
    "                    filename = next(iter(file_upload.value.keys()))\n",
    "                    file_content = file_upload.value[filename]['content']\n",
    "\n",
    "                    # Create a wrapper object with name and content attributes\n",
    "                    class FileWrapper:\n",
    "                        def __init__(self, content, name):\n",
    "                            self.content = content\n",
    "                            self.name = name\n",
    "\n",
    "                    file_wrapper = FileWrapper(file_content, filename)\n",
    "\n",
    "                    success = analyzer.load_data(file_wrapper)\n",
    "                    if success:\n",
    "                        # Update dropdowns with column names\n",
    "                        sensitive_attr_dropdown.options = analyzer.data.columns.tolist()\n",
    "                        target_dropdown.options = analyzer.data.columns.tolist()\n",
    "                        preprocess_button.disabled = False\n",
    "\n",
    "                        # Display preview of the data\n",
    "                        print(\"Data Preview:\")\n",
    "                        display(analyzer.data.head())\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing uploaded file: {e}\")\n",
    "                    print(\"Please try uploading your file again.\")\n",
    "\n",
    "    # Preprocess handler\n",
    "    def on_preprocess_click(b):\n",
    "        with step2_output:\n",
    "            clear_output()\n",
    "            sensitive_attr = sensitive_attr_dropdown.value\n",
    "            target = target_dropdown.value\n",
    "\n",
    "            if sensitive_attr == target:\n",
    "                print(\"Error: Sensitive attribute and target cannot be the same column.\")\n",
    "                return\n",
    "\n",
    "            analyzer.preprocess_data(sensitive_attr, target)\n",
    "            analyzer.train_model()\n",
    "            analyze_button.disabled = False\n",
    "\n",
    "    # Analyze handler\n",
    "    def on_analyze_click(b):\n",
    "        with step3_output:\n",
    "            clear_output()\n",
    "            metrics = analyzer.calculate_fairness_metrics()\n",
    "\n",
    "            # Display summary metrics\n",
    "            print(\"Fairness Metrics Summary:\")\n",
    "            print(f\"Overall Accuracy: {metrics['overall_accuracy']:.4f}\")\n",
    "            print(f\"Demographic Parity Difference: {metrics['demographic_parity_difference']:.4f}\")\n",
    "            print(f\"Equal Opportunity Difference: {metrics['equal_opportunity_difference']:.4f}\")\n",
    "            print(f\"Equalized Odds Difference: {metrics['equalized_odds_difference']:.4f}\")\n",
    "            print(f\"\\nInterpretation: {metrics['dp_interpretation']}\")\n",
    "\n",
    "            # Create visualizations\n",
    "            analyzer.visualize_results()\n",
    "\n",
    "            # Show detailed report\n",
    "            report = analyzer.generate_summary_report()\n",
    "            display(HTML(f\"<pre>{report}</pre>\"))\n",
    "\n",
    "            save_model_button.disabled = False\n",
    "\n",
    "    # Save model handler\n",
    "    def on_save_model_click(b):\n",
    "        with step3_output:\n",
    "            # Save the model to a file\n",
    "            model_filename = 'fairness_model.pkl'\n",
    "            with open(model_filename, 'wb') as f:\n",
    "                pickle.dump(analyzer.model, f)\n",
    "\n",
    "            # Download the file\n",
    "            files.download(model_filename)\n",
    "            print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    # Connect event handlers\n",
    "    file_upload.observe(on_upload_change, names='value')\n",
    "    preprocess_button.on_click(on_preprocess_click)\n",
    "    analyze_button.on_click(on_analyze_click)\n",
    "    save_model_button.on_click(on_save_model_click)\n",
    "\n",
    "    # Build the UI\n",
    "    upload_box = widgets.VBox([\n",
    "        upload_instructions,\n",
    "        file_upload,\n",
    "        step1_output\n",
    "    ])\n",
    "\n",
    "    preprocess_box = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Select Attributes</h3>\"),\n",
    "        sensitive_attr_dropdown,\n",
    "        target_dropdown,\n",
    "        preprocess_button,\n",
    "        step2_output\n",
    "    ])\n",
    "\n",
    "    analyze_box = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Fairness Analysis</h3>\"),\n",
    "        analyze_button,\n",
    "        save_model_button,\n",
    "        step3_output\n",
    "    ])\n",
    "\n",
    "    # Create tabs for workflow\n",
    "    tab = widgets.Tab()\n",
    "    tab.children = [upload_box, preprocess_box, analyze_box]\n",
    "    tab.set_title(0, 'Upload Data')\n",
    "    tab.set_title(1, 'Select Attributes')\n",
    "    tab.set_title(2, 'Analyze Fairness')\n",
    "\n",
    "    # Display the app\n",
    "    display(widgets.HTML(\"<h1>Dataset Fairness Analysis Tool</h1>\"))\n",
    "    display(tab)\n",
    "\n",
    "# Run the app\n",
    "create_fairness_app()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM75PiMGk1eeNXKEU02tMNa",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
